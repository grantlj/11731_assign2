#   generate the vocab book
python vocab.py --train-src=data_ted/all.en-aztr.aztr.txt --train-tgt=data_ted/all.en-aztr.en.txt --size=150000 data_ted/vocab/en-aztr.vocab

python vocab.py --train-src=data_ted/all.en-glpt.glpt.txt --train-tgt=data_ted/all.en-glpt.en.txt --size=150000 data_ted/vocab/en-glpt.vocab

python vocab.py --train-src=data_ted/all.en-beru.beru.txt --train-tgt=data_ted/all.en-beru.en.txt --size=150000 data_ted/vocab/en-beru.vocab


#   generate the vocab book based on subword-nmt
python get_vocab.py --input ../data_ted/all.en-aztr.aztr.txt --output ../data_ted/vocab/aztr.subword.vocab
python get_vocab.py --input ../data_ted/all.en-beru.beru.txt --output ../data_ted/vocab/beru.subword.vocab
python get_vocab.py --input ../data_ted/all.en-glpt.glpt.txt --output ../data_ted/vocab/glpt.subword.vocab

#   train the baseline (aztr)
python nmt.py train --cuda \
--vocab "../data_ted/vocab/en-aztr.vocab" \
--train-src "../data_ted/train.en-aztr.aztr.txt" \
--train-tgt "../data_ted/train.en-aztr.en.txt" \
--dev-src "../data_ted/dev.en-az.az.txt" \
--dev-tgt "../data_ted/dev.en-az.en.txt" \
 --save-to "../models/base_birnn/model_en-aztr.bin" \
  --valid-niter 2000 --batch-size 36 --hidden-size 256 \
  --embed-size 300 --uniform-init 0.1 --dropout 0.2 --clip-grad 5.0 --lr-decay 0.5 | tee aztr.log

#   lowercase
python nmt.py train --cuda \
--vocab "../data_ted/vocab/en-aztr.vocab" \
--train-src "../data_ted/train.en-aztr.aztr.txt" \
--train-tgt "../data_ted/train.en-aztr.en.txt" \
--dev-src "../data_ted/dev.en-az.az.txt" \
--dev-tgt "../data_ted/dev.en-az.en.txt" \
 --save-to "../models/base_birnn_lower/model_en-aztr.bin" \
  --valid-niter 2000 --batch-size 36 --hidden-size 256 \
  --embed-size 300 --uniform-init 0.1 --dropout 0.2 --clip-grad 5.0 --lr-decay 0.5 | tee aztr.log

#   decode the baseline (aztr)
python nmt.py decode --cuda \
--vocab "../data_ted/vocab/en-aztr.vocab" \
--embed-size 300 --hidden-size 256 --dropout 0.2 --beam-size 50 --max-decoding-time-step 100 \
../models/base_birnn/model_en-aztr.bin ../data_ted/test.en-az.az.txt ../data_ted/test.en-az.en.txt \
../output/decode_base_birnn_en-az.en.txt

python nmt.py decode --cuda \
--vocab "../data_ted/vocab/en-aztr.vocab" \
--embed-size 300 --hidden-size 256 --dropout 0.2 --beam-size 50 --max-decoding-time-step 100 \
../models/base_birnn/model_en-aztr.bin ../data_ted/test.en-az.az.txt ../data_ted/test.en-az.en.txt \
../output/decode_base_birnn_en-az.en.lower.txt


#   train the baseline (glpt)
python nmt.py train --cuda \
--vocab "../data_ted/vocab/en-glpt.vocab" \
--train-src "../data_ted/train.en-glpt.glpt.txt" \
--train-tgt "../data_ted/train.en-glpt.en.txt" \
--dev-src "../data_ted/dev.en-gl.gl.txt" \
--dev-tgt "../data_ted/dev.en-gl.en.txt" \
 --save-to "../models/base_birnn/model_en-glpt.bin" \
  --valid-niter 2000 --batch-size 36 --hidden-size 256 \
  --embed-size 300 --uniform-init 0.1 --dropout 0.2 --clip-grad 5.0 --lr-decay 0.5 | tee glpt.log

#   lowercase
python nmt.py train --cuda \
--vocab "../data_ted/vocab/en-glpt.vocab" \
--train-src "../data_ted/train.en-glpt.glpt.txt" \
--train-tgt "../data_ted/train.en-glpt.en.txt" \
--dev-src "../data_ted/dev.en-gl.gl.txt" \
--dev-tgt "../data_ted/dev.en-gl.en.txt" \
 --save-to "../models/base_birnn_lower/model_en-glpt.bin" \
  --valid-niter 2000 --batch-size 36 --hidden-size 256 \
  --embed-size 300 --uniform-init 0.1 --dropout 0.2 --clip-grad 5.0 --lr-decay 0.5 | tee glpt.log


#   decode the baseline (glpt)
python nmt.py decode --cuda \
--vocab "../data_ted/vocab/en-glpt.vocab" \
--embed-size 300 --hidden-size 256 --dropout 0.2 --beam-size 50 --max-decoding-time-step 100 \
../models/base_birnn/model_en-glpt.bin ../data_ted/test.en-gl.gl.txt ../data_ted/test.en-gl.en.txt \
../output/decode_base_birnn_en-gl.en.txt

python nmt.py decode --cuda \
--vocab "../data_ted/vocab/en-glpt.vocab" \
--embed-size 300 --hidden-size 256 --dropout 0.2 --beam-size 50 --max-decoding-time-step 100 \
../models/base_birnn/model_en-glpt.bin ../data_ted/test.en-gl.gl.txt ../data_ted/test.en-gl.en.txt \
../output/decode_base_birnn_en-gl.en.lower.txt


#   train the baseline (beru)
python nmt.py train --cuda \
--vocab "../data_ted/vocab/en-beru.vocab" \
--train-src "../data_ted/train.en-beru.beru.txt" \
--train-tgt "../data_ted/train.en-beru.en.txt" \
--dev-src "../data_ted/dev.en-be.be.txt" \
--dev-tgt "../data_ted/dev.en-be.en.txt" \
 --save-to "../models/base_birnn/model_en-beru.bin" \
  --valid-niter 2000 --batch-size 36 --hidden-size 256 \
  --embed-size 300 --uniform-init 0.1 --dropout 0.2 --clip-grad 5.0 --lr-decay 0.5 | tee beru.log

#   lower
python nmt.py train --cuda \
--vocab "../data_ted/vocab/en-beru.vocab" \
--train-src "../data_ted/train.en-beru.beru.txt" \
--train-tgt "../data_ted/train.en-beru.en.txt" \
--dev-src "../data_ted/dev.en-be.be.txt" \
--dev-tgt "../data_ted/dev.en-be.en.txt" \
 --save-to "../models/base_birnn_lower/model_en-beru.bin" \
  --valid-niter 2000 --batch-size 36 --hidden-size 256 \
  --embed-size 300 --uniform-init 0.1 --dropout 0.2 --clip-grad 5.0 --lr-decay 0.5 | tee beru.log


#   decode the baseline (beru)
python nmt.py decode --cuda \
--vocab "../data_ted/vocab/en-beru.vocab" \
--embed-size 300 --hidden-size 256 --dropout 0.2 --beam-size 50 --max-decoding-time-step 100 \
../models/base_birnn/model_en-beru.bin ../data_ted/test.en-be.be.txt ../data_ted/test.en-be.en.txt \
../output/decode_base_birnn_en-be.en.txt

#   lower
python nmt.py decode --cuda \
--vocab "../data_ted/vocab/en-beru.vocab" \
--embed-size 300 --hidden-size 256 --dropout 0.2 --beam-size 50 --max-decoding-time-step 100 \
../models/base_birnn/model_en-beru.bin ../data_ted/test.en-be.be.txt ../data_ted/test.en-be.en.txt \
../output/decode_base_birnn_en-be.en.lower.txt